{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MLP.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPx7swjSOFgO4Cs6iJJQ5pL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"c99EvWo1s9-x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621309230862,"user_tz":-420,"elapsed":1052,"user":{"displayName":"Zarets LE","photoUrl":"","userId":"18400968482132978129"}},"outputId":"0dcef57a-c628-4319-bc19-ed3c468780dd"},"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","\n","# This will prompt for authorization.\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PJmwSjilXawN","executionInfo":{"status":"ok","timestamp":1621309233060,"user_tz":-420,"elapsed":3222,"user":{"displayName":"Zarets LE","photoUrl":"","userId":"18400968482132978129"}}},"source":["import tensorflow as tf\n","import numpy as np\n","import random"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sj7RYvQKXm_6","executionInfo":{"status":"ok","timestamp":1621309233063,"user_tz":-420,"elapsed":3211,"user":{"displayName":"Zarets LE","photoUrl":"","userId":"18400968482132978129"}}},"source":["class MLP:\n","    def __init__(self, vocab_size, hidden_size):\n","        self._vocab_size = vocab_size\n","        self._hidden_size = hidden_size\n","\n","    def build_graph(self):\n","        self._X = tf.compat.v1.placeholder(tf.float32, shape=[None, self._vocab_size])\n","        self._real_Y = tf.compat.v1.placeholder(tf.int32, shape=[None, ])\n","        NUM_CLASSES = 20\n","\n","        weights_1 = tf.compat.v1.get_variable(\n","            name='weight_input_hidden',\n","            shape=(self._vocab_size, self._hidden_size),\n","            initializer=tf.random_normal_initializer(seed=2020),\n","        )\n","        biases_1 = tf.compat.v1.get_variable(\n","            name='biases_input_hidden',\n","            shape=(self._hidden_size),\n","            initializer=tf.random_normal_initializer(seed=2020)\n","        )\n","\n","        weights_2 = tf.compat.v1.get_variable(\n","            name='weight_hidden_output',\n","            shape=(self._hidden_size, NUM_CLASSES),\n","            initializer=tf.random_normal_initializer(seed=2020),\n","        )\n","\n","        biases_2 = tf.compat.v1.get_variable(\n","            name='biases_hidden_output',\n","            shape=(NUM_CLASSES),\n","            initializer=tf.random_normal_initializer(seed=2020),\n","        )\n","\n","        hidden = tf.matmul(self._X, weights_1) + biases_1\n","        hidden = tf.sigmoid(hidden)\n","        logits = tf.matmul(hidden, weights_2) + biases_2\n","\n","        labels_one_hot = tf.one_hot(indices=self._real_Y,\n","                                    depth=NUM_CLASSES,\n","                                    dtype=tf.float32)\n","\n","        loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels_one_hot,\n","                                                       logits=logits)\n","\n","        loss = tf.reduce_mean(loss)\n","\n","        probs = tf.nn.softmax(logits)\n","        predicted_labels = tf.argmax(probs, axis=1)\n","        predicted_labels = tf.squeeze(predicted_labels)\n","\n","        return predicted_labels, loss\n","\n","    def trainer(self, loss, learning_rate):\n","        train_op = tf.compat.v1.train.AdamOptimizer(learning_rate).minimize(loss)\n","        return train_op"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"mFukZjLpXw3d","executionInfo":{"status":"ok","timestamp":1621309233065,"user_tz":-420,"elapsed":3199,"user":{"displayName":"Zarets LE","photoUrl":"","userId":"18400968482132978129"}}},"source":["class DataReader:\n","    def __init__(self, data_path, batch_size, vocab_size):\n","        self._batch_size = batch_size\n","        with open(data_path) as f:\n","            d_lines = f.read().splitlines()\n","\n","        self._data = []\n","        self._labels = []\n","\n","        for data_id, line in enumerate(d_lines):\n","            vector = [0.0 for _ in range(vocab_size)]\n","            features = line.split('<fff>')\n","            label, doc_id = int(features[0]), int(features[1])\n","            tokens = features[2].split()\n","            for token in tokens:\n","                index, value = int(token.split(':')[0]), float(token.split(':')[1])\n","                vector[index] = value\n","            self._data.append(vector)\n","            self._labels.append(label)\n","\n","        self._data = np.array(self._data)\n","        self._labels = np.array(self._labels)\n","\n","        self._num_epoch = 0\n","        self._batch_id = 0\n","\n","    def next_batch(self):\n","        start = self._batch_id * self._batch_size\n","        end = start + self._batch_size\n","        self._batch_id += 1\n","\n","        if end + self._batch_size > len(self._data):\n","            end = len(self._data)\n","            self._num_epoch += 1\n","            self._batch_id = 0\n","            indices = list(range(len(self._data)))\n","            random.seed(2020)\n","            random.shuffle(indices)\n","            self._data, self._labels = self._data[indices], self._labels[indices]\n","\n","        return self._data[start:end], self._labels[start:end]"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"rQN4gGM6YaVZ","executionInfo":{"status":"ok","timestamp":1621309233068,"user_tz":-420,"elapsed":3188,"user":{"displayName":"Zarets LE","photoUrl":"","userId":"18400968482132978129"}}},"source":["def load_dataset():\n","    train_data_reader = DataReader(\n","        data_path='/content/drive/MyDrive/Session3/20news-train-tfidf.txt',\n","        batch_size=50,\n","        vocab_size=vocab_size\n","    )\n","    test_data_reader = DataReader(\n","        data_path='/content/drive/MyDrive/Session3/20news-test-tfidf.txt',\n","        batch_size=50,\n","        vocab_size=vocab_size\n","    )\n","    return train_data_reader, test_data_reader"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"GKucMD8saj_W","executionInfo":{"status":"ok","timestamp":1621309233071,"user_tz":-420,"elapsed":3175,"user":{"displayName":"Zarets LE","photoUrl":"","userId":"18400968482132978129"}}},"source":["def save_parameters(name, value, epoch):\n","    filename = name.replace(':', '-colon-') + '-epoch-{}.txt'.format(epoch)\n","    if len(value.shape) == 1:\n","        string_form = ','.join([str(number) for number in value])\n","    else:\n","        string_form = '\\n'.join([','.join([str(number) for number in value[row]]) for row in range(value.shape[0])])\n","\n","    with open('/content/drive/MyDrive/Session3/saved-paras/' + filename, \"w\") as f:\n","        f.write(string_form)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"MOV1Bp9Ga5mD","executionInfo":{"status":"ok","timestamp":1621309233074,"user_tz":-420,"elapsed":3164,"user":{"displayName":"Zarets LE","photoUrl":"","userId":"18400968482132978129"}}},"source":["def restore_parameters(name, epoch):\n","    # use saved parameters\n","    filename = name.replace(':', '-colon-') + '-epoch-{}.txt'.format(epoch)\n","    with open('/content/drive/MyDrive/Session3/saved-paras/' + filename) as f:\n","        lines = f.read().splitlines()\n","    if len(lines) == 1:\n","        value = [float(number) for number in lines[0].split(',')]\n","    else:\n","        value = [[float(number) for number in lines[row].split(',')] for row in range(len(lines))]\n","    return value"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ez99xnubbTK5"},"source":["Main:"]},{"cell_type":"code","metadata":{"id":"HrlhN8TdbCYq","executionInfo":{"status":"ok","timestamp":1621309233076,"user_tz":-420,"elapsed":3150,"user":{"displayName":"Zarets LE","photoUrl":"","userId":"18400968482132978129"}}},"source":["tf.compat.v1.disable_eager_execution()"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"fkeoD1SnbeKa","executionInfo":{"status":"ok","timestamp":1621309233078,"user_tz":-420,"elapsed":3133,"user":{"displayName":"Zarets LE","photoUrl":"","userId":"18400968482132978129"}}},"source":["# Create a computation graph\n","with open('/content/drive/MyDrive/Session3/words_idfs.txt') as f:\n","    vocab_size = len(f.read().splitlines())\n","mlp = MLP(\n","    vocab_size=vocab_size,\n","    hidden_size=50\n",")\n","predicted_labels, loss = mlp.build_graph()\n","train_op = mlp.trainer(loss=loss, learning_rate=0.1)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mOjM2tMMgVzV","executionInfo":{"status":"ok","timestamp":1621309895192,"user_tz":-420,"elapsed":665234,"user":{"displayName":"Zarets LE","photoUrl":"","userId":"18400968482132978129"}},"outputId":"9e978a3a-d08e-4fcc-eb66-24c532c57b0a"},"source":["# Open a session to run\n","with tf.compat.v1.Session() as sess:\n","    train_data_reader, test_data_reader = load_dataset()\n","    step, MAX_STEP = 0, 1000\n","\n","    sess.run(tf.compat.v1.global_variables_initializer())\n","    while step <= MAX_STEP:\n","        train_data, train_labels = train_data_reader.next_batch()\n","        plabels_eval, loss_eval, _ = sess.run(\n","            [predicted_labels, loss, train_op],\n","            feed_dict={\n","                mlp._X: train_data,\n","                mlp._real_Y: train_labels\n","            }\n","        )\n","        step += 1\n","        print('step: {}, loss: {}'.format(step, loss_eval))\n","\n","        # save parameters\n","        trainable_variables = tf.compat.v1.trainable_variables()\n","        for variable in trainable_variables:\n","            save_parameters(\n","                name=variable.name,\n","                value=variable.eval(),\n","                epoch=train_data_reader._num_epoch\n","            )"],"execution_count":10,"outputs":[{"output_type":"stream","text":["step: 1, loss: 2.8072032928466797\n","step: 2, loss: 0.05456365644931793\n","step: 3, loss: 0.0005230632959865034\n","step: 4, loss: 9.753648555488326e-06\n","step: 5, loss: 5.078313733974937e-07\n","step: 6, loss: 0.0\n","step: 7, loss: 0.0\n","step: 8, loss: 0.0\n","step: 9, loss: 0.0\n","step: 10, loss: 10.867098808288574\n","step: 11, loss: 24.5706844329834\n","step: 12, loss: 20.508913040161133\n","step: 13, loss: 15.349947929382324\n","step: 14, loss: 10.322065353393555\n","step: 15, loss: 5.678048133850098\n","step: 16, loss: 2.1246843338012695\n","step: 17, loss: 0.408387154340744\n","step: 18, loss: 0.10515070706605911\n","step: 19, loss: 0.05951683595776558\n","step: 20, loss: 0.08250648528337479\n","step: 21, loss: 0.06250625103712082\n","step: 22, loss: 5.287806510925293\n","step: 23, loss: 6.58612060546875\n","step: 24, loss: 5.132270336151123\n","step: 25, loss: 4.346368312835693\n","step: 26, loss: 3.3659327030181885\n","step: 27, loss: 2.869305372238159\n","step: 28, loss: 2.481956720352173\n","step: 29, loss: 2.276973009109497\n","step: 30, loss: 1.9516394138336182\n","step: 31, loss: 1.7337511777877808\n","step: 32, loss: 1.5681856870651245\n","step: 33, loss: 1.6202508211135864\n","step: 34, loss: 3.4559326171875\n","step: 35, loss: 3.48521089553833\n","step: 36, loss: 3.381096839904785\n","step: 37, loss: 3.311279296875\n","step: 38, loss: 3.1573777198791504\n","step: 39, loss: 3.0258102416992188\n","step: 40, loss: 2.7773385047912598\n","step: 41, loss: 2.6038129329681396\n","step: 42, loss: 2.503486394882202\n","step: 43, loss: 2.335111141204834\n","step: 44, loss: 2.1185524463653564\n","step: 45, loss: 2.2401649951934814\n","step: 46, loss: 4.756680965423584\n","step: 47, loss: 4.589242458343506\n","step: 48, loss: 4.565115928649902\n","step: 49, loss: 4.457446098327637\n","step: 50, loss: 4.245655536651611\n","step: 51, loss: 4.052364349365234\n","step: 52, loss: 3.916388511657715\n","step: 53, loss: 3.690006732940674\n","step: 54, loss: 3.4802980422973633\n","step: 55, loss: 3.330430269241333\n","step: 56, loss: 3.134265184402466\n","step: 57, loss: 4.193748950958252\n","step: 58, loss: 5.1930012702941895\n","step: 59, loss: 5.0702924728393555\n","step: 60, loss: 4.945065021514893\n","step: 61, loss: 4.736425876617432\n","step: 62, loss: 4.5528950691223145\n","step: 63, loss: 4.388242721557617\n","step: 64, loss: 4.180204391479492\n","step: 65, loss: 3.9697153568267822\n","step: 66, loss: 3.7976880073547363\n","step: 67, loss: 3.605635404586792\n","step: 68, loss: 3.375358819961548\n","step: 69, loss: 4.745988368988037\n","step: 70, loss: 5.375111103057861\n","step: 71, loss: 5.283606052398682\n","step: 72, loss: 5.1163105964660645\n","step: 73, loss: 4.922410488128662\n","step: 74, loss: 4.748296737670898\n","step: 75, loss: 4.565627574920654\n","step: 76, loss: 4.353121280670166\n","step: 77, loss: 4.147705554962158\n","step: 78, loss: 3.9603497982025146\n","step: 79, loss: 3.750375747680664\n","step: 80, loss: 3.581622362136841\n","step: 81, loss: 5.6590986251831055\n","step: 82, loss: 5.602757453918457\n","step: 83, loss: 5.4396071434021\n","step: 84, loss: 5.288382530212402\n","step: 85, loss: 5.108002185821533\n","step: 86, loss: 4.9120965003967285\n","step: 87, loss: 4.723426342010498\n","step: 88, loss: 4.5089240074157715\n","step: 89, loss: 4.304446220397949\n","step: 90, loss: 4.080966949462891\n","step: 91, loss: 3.8711352348327637\n","step: 92, loss: 3.881800889968872\n","step: 93, loss: 5.620954513549805\n","step: 94, loss: 5.489397048950195\n","step: 95, loss: 5.339552402496338\n","step: 96, loss: 5.1641998291015625\n","step: 97, loss: 4.977553844451904\n","step: 98, loss: 4.776037216186523\n","step: 99, loss: 4.574784755706787\n","step: 100, loss: 4.3617844581604\n","step: 101, loss: 4.144840240478516\n","step: 102, loss: 3.919229030609131\n","step: 103, loss: 3.703706741333008\n","step: 104, loss: 3.838345527648926\n","step: 105, loss: 5.916048526763916\n","step: 106, loss: 5.791281223297119\n","step: 107, loss: 5.632139205932617\n","step: 108, loss: 5.476353168487549\n","step: 109, loss: 5.244354248046875\n","step: 110, loss: 5.056897163391113\n","step: 111, loss: 4.8260650634765625\n","step: 112, loss: 4.6162919998168945\n","step: 113, loss: 4.385661602020264\n","step: 114, loss: 4.149061679840088\n","step: 115, loss: 3.921273708343506\n","step: 116, loss: 4.1679792404174805\n","step: 117, loss: 5.97377872467041\n","step: 118, loss: 5.835247039794922\n","step: 119, loss: 5.668581485748291\n","step: 120, loss: 5.4870524406433105\n","step: 121, loss: 5.278292655944824\n","step: 122, loss: 5.0696258544921875\n","step: 123, loss: 4.842154026031494\n","step: 124, loss: 4.61080265045166\n","step: 125, loss: 4.378839015960693\n","step: 126, loss: 4.137947082519531\n","step: 127, loss: 3.8975183963775635\n","step: 128, loss: 4.1869282722473145\n","step: 129, loss: 6.206863880157471\n","step: 130, loss: 6.070301532745361\n","step: 131, loss: 5.901086807250977\n","step: 132, loss: 5.700170993804932\n","step: 133, loss: 5.4909257888793945\n","step: 134, loss: 5.268190860748291\n","step: 135, loss: 5.037090301513672\n","step: 136, loss: 4.798469543457031\n","step: 137, loss: 4.556364059448242\n","step: 138, loss: 4.3098320960998535\n","step: 139, loss: 4.0589776039123535\n","step: 140, loss: 4.563980579376221\n","step: 141, loss: 6.20499324798584\n","step: 142, loss: 6.053882598876953\n","step: 143, loss: 5.870680332183838\n","step: 144, loss: 5.666087627410889\n","step: 145, loss: 5.448135852813721\n","step: 146, loss: 5.218559741973877\n","step: 147, loss: 4.978044033050537\n","step: 148, loss: 4.728673934936523\n","step: 149, loss: 4.475336074829102\n","step: 150, loss: 4.218623161315918\n","step: 152, loss: 5.0265116691589355\n","step: 153, loss: 6.33355712890625\n","step: 154, loss: 6.167403697967529\n","step: 155, loss: 5.9791789054870605\n","step: 156, loss: 5.758121967315674\n","step: 157, loss: 5.52808952331543\n","step: 158, loss: 5.293925762176514\n","step: 159, loss: 5.039178848266602\n","step: 160, loss: 4.78065299987793\n","step: 161, loss: 4.52030611038208\n","step: 162, loss: 4.254411697387695\n","step: 163, loss: 3.9891321659088135\n","step: 164, loss: 5.356692314147949\n","step: 165, loss: 6.328995227813721\n","step: 166, loss: 6.155035018920898\n","step: 167, loss: 5.954554557800293\n","step: 168, loss: 5.7407636642456055\n","step: 169, loss: 5.494826793670654\n","step: 170, loss: 5.244598865509033\n","step: 171, loss: 4.991690158843994\n","step: 172, loss: 4.725322723388672\n","step: 173, loss: 4.453716278076172\n","step: 174, loss: 4.181445121765137\n","step: 175, loss: 3.910768985748291\n","step: 176, loss: 5.761668682098389\n","step: 177, loss: 6.3629279136657715\n","step: 178, loss: 6.181487560272217\n","step: 179, loss: 5.975596904754639\n","step: 180, loss: 5.744381904602051\n","step: 181, loss: 5.501518726348877\n","step: 182, loss: 5.244560718536377\n","step: 183, loss: 4.978527545928955\n","step: 184, loss: 4.7085676193237305\n","step: 185, loss: 4.432466983795166\n","step: 186, loss: 4.153951644897461\n","step: 187, loss: 3.877356767654419\n","step: 188, loss: 6.014142036437988\n","step: 189, loss: 6.642621040344238\n","step: 190, loss: 6.461160659790039\n","step: 191, loss: 6.248188018798828\n","step: 192, loss: 6.014956474304199\n","step: 193, loss: 5.76634407043457\n","step: 194, loss: 5.502889633178711\n","step: 195, loss: 5.232941150665283\n","step: 196, loss: 4.953712463378906\n","step: 197, loss: 4.671763896942139\n","step: 198, loss: 4.385046005249023\n","step: 199, loss: 6.358520984649658\n","step: 200, loss: 6.650274276733398\n","step: 201, loss: 6.4625959396362305\n","step: 202, loss: 6.239459037780762\n","step: 203, loss: 5.996222019195557\n","step: 204, loss: 5.73924446105957\n","step: 205, loss: 5.467939376831055\n","step: 206, loss: 5.1848602294921875\n","step: 207, loss: 4.893082141876221\n","step: 208, loss: 4.603465557098389\n","step: 209, loss: 4.303458213806152\n","step: 210, loss: 5.527822971343994\n","step: 211, loss: 6.58213996887207\n","step: 212, loss: 6.3964619636535645\n","step: 213, loss: 6.1680145263671875\n","step: 214, loss: 5.922688007354736\n","step: 215, loss: 5.656701564788818\n","step: 216, loss: 5.380545616149902\n","step: 217, loss: 5.094700813293457\n","step: 218, loss: 4.805094242095947\n","step: 219, loss: 5.133224010467529\n","step: 220, loss: 6.7826247215271\n","step: 221, loss: 6.611422061920166\n","step: 222, loss: 6.396307468414307\n","step: 223, loss: 6.152010440826416\n","step: 224, loss: 5.890718460083008\n","step: 225, loss: 5.614286422729492\n","step: 226, loss: 3.1260409355163574\n","step: 227, loss: 2.9839119911193848\n","step: 228, loss: 3.366367816925049\n","step: 229, loss: 3.154383659362793\n","step: 230, loss: 3.2392096519470215\n","step: 231, loss: 3.150946617126465\n","step: 232, loss: 3.1166090965270996\n","step: 233, loss: 3.0839178562164307\n","step: 234, loss: 3.0930700302124023\n","step: 235, loss: 3.2270421981811523\n","step: 236, loss: 3.180987596511841\n","step: 237, loss: 3.0788748264312744\n","step: 238, loss: 3.0937371253967285\n","step: 239, loss: 3.1424572467803955\n","step: 240, loss: 3.1667850017547607\n","step: 241, loss: 3.168503522872925\n","step: 242, loss: 3.2471225261688232\n","step: 243, loss: 3.2634148597717285\n","step: 244, loss: 3.1872622966766357\n","step: 245, loss: 3.0892386436462402\n","step: 246, loss: 3.0102148056030273\n","step: 247, loss: 3.208059072494507\n","step: 248, loss: 3.010298728942871\n","step: 249, loss: 3.10745906829834\n","step: 250, loss: 3.0503106117248535\n","step: 251, loss: 3.172013282775879\n","step: 252, loss: 3.0817718505859375\n","step: 253, loss: 3.088270664215088\n","step: 254, loss: 3.153759241104126\n","step: 255, loss: 3.082268714904785\n","step: 256, loss: 3.112231492996216\n","step: 257, loss: 3.0592610836029053\n","step: 258, loss: 3.079512596130371\n","step: 259, loss: 3.0945019721984863\n","step: 260, loss: 3.021408796310425\n","step: 261, loss: 3.0707483291625977\n","step: 262, loss: 3.0466573238372803\n","step: 263, loss: 2.9989283084869385\n","step: 264, loss: 3.0717029571533203\n","step: 265, loss: 3.0192863941192627\n","step: 266, loss: 2.994852066040039\n","step: 267, loss: 3.006913423538208\n","step: 268, loss: 3.0094079971313477\n","step: 269, loss: 3.019850492477417\n","step: 270, loss: 3.042048931121826\n","step: 271, loss: 3.0002336502075195\n","step: 272, loss: 3.0005459785461426\n","step: 273, loss: 3.026139497756958\n","step: 274, loss: 2.978264570236206\n","step: 275, loss: 3.000357151031494\n","step: 276, loss: 2.9800140857696533\n","step: 277, loss: 2.999401330947876\n","step: 278, loss: 3.003937005996704\n","step: 279, loss: 3.009694814682007\n","step: 280, loss: 3.0050933361053467\n","step: 281, loss: 2.9799811840057373\n","step: 282, loss: 2.963881254196167\n","step: 283, loss: 3.006883144378662\n","step: 284, loss: 2.9581475257873535\n","step: 285, loss: 2.9949159622192383\n","step: 286, loss: 3.0165982246398926\n","step: 287, loss: 2.9775280952453613\n","step: 288, loss: 2.994797706604004\n","step: 289, loss: 2.9943549633026123\n","step: 290, loss: 2.9928107261657715\n","step: 291, loss: 3.0077381134033203\n","step: 292, loss: 2.972452402114868\n","step: 293, loss: 3.0028553009033203\n","step: 294, loss: 2.9948456287384033\n","step: 295, loss: 2.978043794631958\n","step: 296, loss: 3.0109221935272217\n","step: 297, loss: 3.001082181930542\n","step: 298, loss: 2.9848809242248535\n","step: 299, loss: 3.001222848892212\n","step: 300, loss: 3.0046961307525635\n","step: 301, loss: 2.9815235137939453\n","step: 302, loss: 2.9964544773101807\n","step: 303, loss: 2.9940686225891113\n","step: 304, loss: 2.9666152000427246\n","step: 305, loss: 3.028855562210083\n","step: 306, loss: 3.0080273151397705\n","step: 307, loss: 2.9818243980407715\n","step: 308, loss: 2.9847402572631836\n","step: 309, loss: 2.9867746829986572\n","step: 310, loss: 3.012340784072876\n","step: 311, loss: 2.973261833190918\n","step: 312, loss: 2.997481346130371\n","step: 313, loss: 2.9872403144836426\n","step: 314, loss: 2.9833672046661377\n","step: 315, loss: 2.9785430431365967\n","step: 316, loss: 2.9906418323516846\n","step: 317, loss: 2.991515874862671\n","step: 318, loss: 2.9892141819000244\n","step: 319, loss: 2.9935808181762695\n","step: 320, loss: 3.000168561935425\n","step: 321, loss: 2.9965028762817383\n","step: 322, loss: 2.9780709743499756\n","step: 323, loss: 3.0165457725524902\n","step: 324, loss: 2.988166570663452\n","step: 325, loss: 2.9976794719696045\n","step: 326, loss: 2.996378183364868\n","step: 327, loss: 2.9899117946624756\n","step: 328, loss: 2.9800238609313965\n","step: 329, loss: 2.99300479888916\n","step: 330, loss: 2.9988317489624023\n","step: 331, loss: 3.0115294456481934\n","step: 332, loss: 2.9827377796173096\n","step: 333, loss: 3.0196638107299805\n","step: 334, loss: 2.9815869331359863\n","step: 335, loss: 2.9744133949279785\n","step: 336, loss: 2.995063066482544\n","step: 337, loss: 2.9544365406036377\n","step: 338, loss: 2.977658748626709\n","step: 339, loss: 2.994807243347168\n","step: 340, loss: 3.007661819458008\n","step: 341, loss: 2.97053599357605\n","step: 342, loss: 2.973673105239868\n","step: 343, loss: 2.961050033569336\n","step: 344, loss: 2.986234188079834\n","step: 345, loss: 3.0134782791137695\n","step: 346, loss: 3.0230541229248047\n","step: 347, loss: 2.9856326580047607\n","step: 348, loss: 3.0002753734588623\n","step: 349, loss: 2.9997358322143555\n","step: 350, loss: 2.973405122756958\n","step: 351, loss: 2.969707727432251\n","step: 352, loss: 2.996798038482666\n","step: 353, loss: 2.9243059158325195\n","step: 354, loss: 2.979409694671631\n","step: 355, loss: 2.9767370223999023\n","step: 356, loss: 2.9772789478302\n","step: 357, loss: 2.999955654144287\n","step: 358, loss: 2.956382989883423\n","step: 359, loss: 2.989150047302246\n","step: 360, loss: 2.973860502243042\n","step: 361, loss: 3.0108940601348877\n","step: 362, loss: 2.9642322063446045\n","step: 363, loss: 2.990133762359619\n","step: 364, loss: 3.007443428039551\n","step: 365, loss: 2.9943084716796875\n","step: 366, loss: 2.981504201889038\n","step: 367, loss: 2.989467144012451\n","step: 368, loss: 2.976806640625\n","step: 369, loss: 3.0101261138916016\n","step: 370, loss: 2.982853412628174\n","step: 371, loss: 2.995668888092041\n","step: 372, loss: 2.984736442565918\n","step: 373, loss: 2.982855796813965\n","step: 374, loss: 2.950622797012329\n","step: 375, loss: 3.004936456680298\n","step: 376, loss: 2.988632917404175\n","step: 377, loss: 3.0011816024780273\n","step: 378, loss: 2.955836772918701\n","step: 379, loss: 2.9807329177856445\n","step: 380, loss: 3.000195026397705\n","step: 381, loss: 2.9918200969696045\n","step: 382, loss: 2.984226703643799\n","step: 383, loss: 2.998497247695923\n","step: 384, loss: 2.9870147705078125\n","step: 385, loss: 3.0007781982421875\n","step: 386, loss: 2.981894016265869\n","step: 387, loss: 2.9555623531341553\n","step: 388, loss: 2.9788830280303955\n","step: 389, loss: 2.997182607650757\n","step: 390, loss: 2.9914045333862305\n","step: 391, loss: 3.0025501251220703\n","step: 392, loss: 2.9852211475372314\n","step: 393, loss: 3.013881206512451\n","step: 394, loss: 2.991635799407959\n","step: 395, loss: 2.976161479949951\n","step: 396, loss: 2.995623826980591\n","step: 397, loss: 2.996537446975708\n","step: 398, loss: 2.991717576980591\n","step: 399, loss: 2.973304510116577\n","step: 400, loss: 3.004830837249756\n","step: 401, loss: 3.020319938659668\n","step: 402, loss: 3.0141777992248535\n","step: 403, loss: 2.969838857650757\n","step: 404, loss: 2.9949569702148438\n","step: 405, loss: 2.992795944213867\n","step: 406, loss: 2.976119041442871\n","step: 407, loss: 2.9909863471984863\n","step: 408, loss: 3.0193397998809814\n","step: 409, loss: 3.006619453430176\n","step: 410, loss: 2.997927188873291\n","step: 411, loss: 2.9965381622314453\n","step: 412, loss: 2.9915103912353516\n","step: 413, loss: 2.9954421520233154\n","step: 414, loss: 2.9825732707977295\n","step: 415, loss: 2.957531690597534\n","step: 416, loss: 2.9777441024780273\n","step: 417, loss: 2.994978904724121\n","step: 418, loss: 3.0014595985412598\n","step: 419, loss: 3.0132830142974854\n","step: 420, loss: 2.9862442016601562\n","step: 421, loss: 2.9958319664001465\n","step: 422, loss: 2.9861090183258057\n","step: 423, loss: 2.9806106090545654\n","step: 424, loss: 2.977266311645508\n","step: 425, loss: 2.9879872798919678\n","step: 426, loss: 2.9968390464782715\n","step: 427, loss: 2.9591236114501953\n","step: 428, loss: 2.9836695194244385\n","step: 429, loss: 2.9310522079467773\n","step: 430, loss: 2.9885709285736084\n","step: 431, loss: 2.978633403778076\n","step: 432, loss: 2.995432138442993\n","step: 433, loss: 2.988604784011841\n","step: 434, loss: 2.985877275466919\n","step: 435, loss: 2.9666855335235596\n","step: 436, loss: 2.981576442718506\n","step: 437, loss: 3.0282514095306396\n","step: 438, loss: 2.9767534732818604\n","step: 439, loss: 2.9982540607452393\n","step: 440, loss: 2.972379684448242\n","step: 441, loss: 2.976594924926758\n","step: 442, loss: 2.9652674198150635\n","step: 443, loss: 2.997488498687744\n","step: 444, loss: 3.0003762245178223\n","step: 445, loss: 3.0230748653411865\n","step: 446, loss: 2.9444386959075928\n","step: 447, loss: 3.000671625137329\n","step: 448, loss: 2.9945404529571533\n","step: 449, loss: 3.0000269412994385\n","step: 450, loss: 3.0032095909118652\n","step: 451, loss: 2.9769697189331055\n","step: 452, loss: 2.914668560028076\n","step: 453, loss: 2.958406448364258\n","step: 454, loss: 2.96405029296875\n","step: 455, loss: 2.9702718257904053\n","step: 456, loss: 3.027215003967285\n","step: 457, loss: 2.9731698036193848\n","step: 458, loss: 3.004020929336548\n","step: 459, loss: 2.9589056968688965\n","step: 460, loss: 2.9285593032836914\n","step: 461, loss: 3.004931926727295\n","step: 462, loss: 3.0166373252868652\n","step: 463, loss: 2.9944207668304443\n","step: 464, loss: 2.923715829849243\n","step: 465, loss: 3.003817081451416\n","step: 466, loss: 3.0363881587982178\n","step: 467, loss: 2.9208579063415527\n","step: 468, loss: 2.9739654064178467\n","step: 469, loss: 3.0186331272125244\n","step: 470, loss: 2.9703986644744873\n","step: 471, loss: 3.031620979309082\n","step: 472, loss: 2.9808928966522217\n","step: 473, loss: 3.0023069381713867\n","step: 474, loss: 2.970743417739868\n","step: 475, loss: 2.998375177383423\n","step: 476, loss: 2.9757497310638428\n","step: 477, loss: 2.9478378295898438\n","step: 478, loss: 2.9857919216156006\n","step: 479, loss: 2.8951830863952637\n","step: 480, loss: 3.0413734912872314\n","step: 481, loss: 3.0110855102539062\n","step: 482, loss: 3.1021692752838135\n","step: 483, loss: 2.983306884765625\n","step: 484, loss: 2.9755377769470215\n","step: 485, loss: 3.0728936195373535\n","step: 486, loss: 2.9721081256866455\n","step: 487, loss: 2.995762586593628\n","step: 488, loss: 2.9488251209259033\n","step: 489, loss: 2.976564884185791\n","step: 490, loss: 3.013775110244751\n","step: 491, loss: 2.991580009460449\n","step: 492, loss: 2.9845588207244873\n","step: 493, loss: 2.9732728004455566\n","step: 494, loss: 2.9582574367523193\n","step: 495, loss: 2.977938652038574\n","step: 496, loss: 2.9732141494750977\n","step: 497, loss: 2.919377088546753\n","step: 498, loss: 2.9688143730163574\n","step: 499, loss: 2.959808111190796\n","step: 500, loss: 3.005509853363037\n","step: 501, loss: 2.98463773727417\n","step: 502, loss: 2.905864953994751\n","step: 503, loss: 2.9080071449279785\n","step: 504, loss: 2.9863719940185547\n","step: 505, loss: 2.990600347518921\n","step: 506, loss: 2.9911115169525146\n","step: 507, loss: 2.9701476097106934\n","step: 508, loss: 3.0356054306030273\n","step: 509, loss: 2.992931842803955\n","step: 510, loss: 2.973270893096924\n","step: 511, loss: 2.9185285568237305\n","step: 512, loss: 2.928007125854492\n","step: 513, loss: 2.973726272583008\n","step: 514, loss: 2.9606571197509766\n","step: 515, loss: 2.9507925510406494\n","step: 516, loss: 2.9918386936187744\n","step: 517, loss: 2.924222469329834\n","step: 518, loss: 2.9000906944274902\n","step: 519, loss: 2.9013304710388184\n","step: 520, loss: 2.8634417057037354\n","step: 521, loss: 2.8273932933807373\n","step: 522, loss: 3.00380802154541\n","step: 523, loss: 2.8983874320983887\n","step: 524, loss: 2.898679733276367\n","step: 525, loss: 2.854806423187256\n","step: 526, loss: 2.830950975418091\n","step: 527, loss: 2.83402681350708\n","step: 528, loss: 2.904672622680664\n","step: 529, loss: 2.712766647338867\n","step: 530, loss: 2.8643999099731445\n","step: 531, loss: 2.6981360912323\n","step: 532, loss: 2.8390700817108154\n","step: 533, loss: 2.682105779647827\n","step: 534, loss: 2.889611005783081\n","step: 535, loss: 2.8708856105804443\n","step: 536, loss: 2.91294002532959\n","step: 537, loss: 2.8353078365325928\n","step: 538, loss: 2.6162781715393066\n","step: 539, loss: 2.810062885284424\n","step: 540, loss: 2.6979739665985107\n","step: 541, loss: 2.576850652694702\n","step: 542, loss: 2.739044189453125\n","step: 543, loss: 2.660759687423706\n","step: 544, loss: 2.8899035453796387\n","step: 545, loss: 2.8372461795806885\n","step: 546, loss: 2.7203900814056396\n","step: 547, loss: 2.5563783645629883\n","step: 548, loss: 2.8766674995422363\n","step: 549, loss: 2.6429498195648193\n","step: 550, loss: 2.587073564529419\n","step: 551, loss: 2.5327911376953125\n","step: 552, loss: 2.847607135772705\n","step: 553, loss: 2.569852828979492\n","step: 554, loss: 2.7426364421844482\n","step: 555, loss: 2.7418160438537598\n","step: 556, loss: 2.578781843185425\n","step: 557, loss: 2.674548625946045\n","step: 558, loss: 2.681037664413452\n","step: 559, loss: 2.6235010623931885\n","step: 560, loss: 2.795588493347168\n","step: 561, loss: 2.483048915863037\n","step: 562, loss: 2.7016947269439697\n","step: 563, loss: 2.496943473815918\n","step: 564, loss: 2.604945659637451\n","step: 565, loss: 2.732064723968506\n","step: 566, loss: 2.550044536590576\n","step: 567, loss: 2.5335533618927\n","step: 568, loss: 2.8010451793670654\n","step: 569, loss: 2.7005815505981445\n","step: 570, loss: 2.6865429878234863\n","step: 571, loss: 2.5205752849578857\n","step: 572, loss: 2.780942678451538\n","step: 573, loss: 2.5450875759124756\n","step: 574, loss: 2.45782732963562\n","step: 575, loss: 2.6350619792938232\n","step: 576, loss: 2.4486420154571533\n","step: 577, loss: 2.514829397201538\n","step: 578, loss: 2.408689022064209\n","step: 579, loss: 2.441023349761963\n","step: 580, loss: 2.4153385162353516\n","step: 581, loss: 2.284749984741211\n","step: 582, loss: 2.46738338470459\n","step: 583, loss: 2.3807296752929688\n","step: 584, loss: 2.5270235538482666\n","step: 585, loss: 2.1654398441314697\n","step: 586, loss: 2.465770959854126\n","step: 587, loss: 2.5994150638580322\n","step: 588, loss: 2.1083083152770996\n","step: 589, loss: 2.147186040878296\n","step: 590, loss: 2.289210557937622\n","step: 591, loss: 2.464404582977295\n","step: 592, loss: 2.2653725147247314\n","step: 593, loss: 2.3578450679779053\n","step: 594, loss: 2.0654373168945312\n","step: 595, loss: 2.391288995742798\n","step: 596, loss: 2.4761507511138916\n","step: 597, loss: 2.284456729888916\n","step: 598, loss: 2.095724105834961\n","step: 599, loss: 2.512155055999756\n","step: 600, loss: 2.343672752380371\n","step: 601, loss: 2.3470640182495117\n","step: 602, loss: 2.0737411975860596\n","step: 603, loss: 2.151815891265869\n","step: 604, loss: 2.2248973846435547\n","step: 605, loss: 1.9993643760681152\n","step: 606, loss: 2.1495301723480225\n","step: 607, loss: 2.2795794010162354\n","step: 608, loss: 2.2865045070648193\n","step: 609, loss: 2.1877782344818115\n","step: 610, loss: 2.0399112701416016\n","step: 611, loss: 2.4149060249328613\n","step: 612, loss: 1.9023319482803345\n","step: 613, loss: 2.0102787017822266\n","step: 614, loss: 1.9015851020812988\n","step: 615, loss: 2.093935489654541\n","step: 616, loss: 1.705039620399475\n","step: 617, loss: 2.2219932079315186\n","step: 618, loss: 1.91219162940979\n","step: 619, loss: 2.351501226425171\n","step: 620, loss: 2.015378713607788\n","step: 621, loss: 1.9575066566467285\n","step: 622, loss: 1.9643430709838867\n","step: 623, loss: 2.0013253688812256\n","step: 624, loss: 1.7602052688598633\n","step: 625, loss: 1.9651445150375366\n","step: 626, loss: 2.003906488418579\n","step: 627, loss: 1.9947525262832642\n","step: 628, loss: 2.210632562637329\n","step: 629, loss: 1.7646148204803467\n","step: 630, loss: 2.161024808883667\n","step: 631, loss: 1.8916621208190918\n","step: 632, loss: 1.5232152938842773\n","step: 633, loss: 1.8570196628570557\n","step: 634, loss: 2.0407419204711914\n","step: 635, loss: 1.9945980310440063\n","step: 636, loss: 1.885825514793396\n","step: 637, loss: 2.314037561416626\n","step: 638, loss: 1.7823126316070557\n","step: 639, loss: 1.8871676921844482\n","step: 640, loss: 1.887741208076477\n","step: 641, loss: 2.0112907886505127\n","step: 642, loss: 1.8555749654769897\n","step: 643, loss: 1.9230154752731323\n","step: 644, loss: 1.8738476037979126\n","step: 645, loss: 2.1471030712127686\n","step: 646, loss: 1.9404501914978027\n","step: 647, loss: 1.8881415128707886\n","step: 648, loss: 1.7870036363601685\n","step: 649, loss: 1.8062556982040405\n","step: 650, loss: 1.9735678434371948\n","step: 651, loss: 1.7504922151565552\n","step: 652, loss: 1.82181715965271\n","step: 653, loss: 1.959452509880066\n","step: 654, loss: 1.8373923301696777\n","step: 655, loss: 1.9403916597366333\n","step: 656, loss: 1.7168478965759277\n","step: 657, loss: 2.228921413421631\n","step: 658, loss: 1.7318059206008911\n","step: 659, loss: 1.6791579723358154\n","step: 660, loss: 1.8600274324417114\n","step: 661, loss: 1.577290654182434\n","step: 662, loss: 1.5875705480575562\n","step: 663, loss: 1.7631760835647583\n","step: 664, loss: 1.7446999549865723\n","step: 665, loss: 1.7097539901733398\n","step: 666, loss: 1.6676815748214722\n","step: 667, loss: 1.4726752042770386\n","step: 668, loss: 1.4869627952575684\n","step: 669, loss: 1.5409573316574097\n","step: 670, loss: 1.5642701387405396\n","step: 671, loss: 1.5022659301757812\n","step: 672, loss: 1.6345832347869873\n","step: 673, loss: 1.3916815519332886\n","step: 674, loss: 1.617464542388916\n","step: 675, loss: 1.6449795961380005\n","step: 676, loss: 1.3729854822158813\n","step: 677, loss: 1.7740832567214966\n","step: 678, loss: 1.6921886205673218\n","step: 679, loss: 1.3931573629379272\n","step: 680, loss: 1.464108943939209\n","step: 681, loss: 1.468997836112976\n","step: 682, loss: 1.3179365396499634\n","step: 683, loss: 1.4657734632492065\n","step: 684, loss: 1.1713489294052124\n","step: 685, loss: 1.2175105810165405\n","step: 686, loss: 1.364038109779358\n","step: 687, loss: 1.6505769491195679\n","step: 688, loss: 1.3634692430496216\n","step: 689, loss: 1.5466924905776978\n","step: 690, loss: 1.1919875144958496\n","step: 691, loss: 1.1685137748718262\n","step: 692, loss: 1.7511590719223022\n","step: 693, loss: 1.6386139392852783\n","step: 694, loss: 1.560920238494873\n","step: 695, loss: 1.3406262397766113\n","step: 696, loss: 1.286530613899231\n","step: 697, loss: 1.4539788961410522\n","step: 698, loss: 1.3298298120498657\n","step: 699, loss: 1.1748173236846924\n","step: 700, loss: 1.1870840787887573\n","step: 701, loss: 1.3137294054031372\n","step: 702, loss: 1.3016905784606934\n","step: 703, loss: 1.3125238418579102\n","step: 704, loss: 1.1907267570495605\n","step: 705, loss: 0.9932066202163696\n","step: 706, loss: 1.1606659889221191\n","step: 707, loss: 1.4121066331863403\n","step: 708, loss: 1.1242529153823853\n","step: 709, loss: 1.3730218410491943\n","step: 710, loss: 1.1475147008895874\n","step: 711, loss: 1.2990753650665283\n","step: 712, loss: 1.3720426559448242\n","step: 713, loss: 1.0648739337921143\n","step: 714, loss: 1.3030012845993042\n","step: 715, loss: 1.3499438762664795\n","step: 716, loss: 1.3538516759872437\n","step: 717, loss: 1.1130363941192627\n","step: 718, loss: 1.187178373336792\n","step: 719, loss: 1.0852024555206299\n","step: 720, loss: 0.9940040707588196\n","step: 721, loss: 0.9412583708763123\n","step: 722, loss: 1.5599135160446167\n","step: 723, loss: 0.9255415201187134\n","step: 724, loss: 0.9843738675117493\n","step: 725, loss: 1.4583184719085693\n","step: 726, loss: 1.264838457107544\n","step: 727, loss: 1.040493130683899\n","step: 728, loss: 1.4036527872085571\n","step: 729, loss: 1.2045035362243652\n","step: 730, loss: 1.2052748203277588\n","step: 731, loss: 1.606998324394226\n","step: 732, loss: 1.6598788499832153\n","step: 733, loss: 1.0326488018035889\n","step: 734, loss: 1.2548532485961914\n","step: 735, loss: 1.187991738319397\n","step: 736, loss: 1.3097048997879028\n","step: 737, loss: 1.3451772928237915\n","step: 738, loss: 1.191542625427246\n","step: 739, loss: 1.195298433303833\n","step: 740, loss: 1.1703227758407593\n","step: 741, loss: 1.1735010147094727\n","step: 742, loss: 0.8980291485786438\n","step: 743, loss: 0.830436110496521\n","step: 744, loss: 1.522082805633545\n","step: 745, loss: 1.0756239891052246\n","step: 746, loss: 0.948310375213623\n","step: 747, loss: 1.126062273979187\n","step: 748, loss: 1.3362770080566406\n","step: 749, loss: 1.060603380203247\n","step: 750, loss: 0.9141145348548889\n","step: 751, loss: 1.1194660663604736\n","step: 752, loss: 1.3985289335250854\n","step: 753, loss: 1.2044498920440674\n","step: 754, loss: 1.2204197645187378\n","step: 755, loss: 0.9949014186859131\n","step: 756, loss: 0.9967997670173645\n","step: 757, loss: 1.1454825401306152\n","step: 758, loss: 1.2764135599136353\n","step: 759, loss: 0.6982508897781372\n","step: 760, loss: 0.6898820996284485\n","step: 761, loss: 1.0648061037063599\n","step: 762, loss: 1.2719324827194214\n","step: 763, loss: 1.1903632879257202\n","step: 764, loss: 1.0483547449111938\n","step: 765, loss: 1.1330219507217407\n","step: 766, loss: 0.9717361927032471\n","step: 767, loss: 0.8832075595855713\n","step: 768, loss: 1.0473899841308594\n","step: 769, loss: 0.7998055815696716\n","step: 770, loss: 1.0732159614562988\n","step: 771, loss: 0.8282089233398438\n","step: 772, loss: 0.883290708065033\n","step: 773, loss: 1.318243145942688\n","step: 774, loss: 0.8678793907165527\n","step: 775, loss: 1.1922409534454346\n","step: 776, loss: 0.8483524322509766\n","step: 777, loss: 1.6411055326461792\n","step: 778, loss: 1.0121537446975708\n","step: 779, loss: 0.7908433675765991\n","step: 780, loss: 0.837918758392334\n","step: 781, loss: 1.0034375190734863\n","step: 782, loss: 1.079268455505371\n","step: 783, loss: 0.9836689233779907\n","step: 784, loss: 0.8084351420402527\n","step: 785, loss: 0.7369874715805054\n","step: 786, loss: 1.0803548097610474\n","step: 787, loss: 0.7415724396705627\n","step: 788, loss: 0.9597685933113098\n","step: 789, loss: 1.0215071439743042\n","step: 790, loss: 1.2176802158355713\n","step: 791, loss: 0.8254706859588623\n","step: 792, loss: 0.9109919667243958\n","step: 793, loss: 0.6747294664382935\n","step: 794, loss: 0.6301353573799133\n","step: 795, loss: 1.3653720617294312\n","step: 796, loss: 0.7283729314804077\n","step: 797, loss: 1.1293084621429443\n","step: 798, loss: 0.7883680462837219\n","step: 799, loss: 1.1425204277038574\n","step: 800, loss: 1.0339078903198242\n","step: 801, loss: 1.1746587753295898\n","step: 802, loss: 1.170189380645752\n","step: 803, loss: 1.1029192209243774\n","step: 804, loss: 0.6220026612281799\n","step: 805, loss: 0.8879397511482239\n","step: 806, loss: 0.7501514554023743\n","step: 807, loss: 0.7281244397163391\n","step: 808, loss: 0.6965867877006531\n","step: 809, loss: 0.7088838815689087\n","step: 810, loss: 0.5455398559570312\n","step: 811, loss: 0.9227950572967529\n","step: 812, loss: 0.8878467679023743\n","step: 813, loss: 0.5855251550674438\n","step: 814, loss: 0.6000999212265015\n","step: 815, loss: 0.6071631908416748\n","step: 816, loss: 0.6452987790107727\n","step: 817, loss: 0.7072533965110779\n","step: 818, loss: 0.9221470355987549\n","step: 819, loss: 0.7369186878204346\n","step: 820, loss: 0.7588912844657898\n","step: 821, loss: 0.5392207503318787\n","step: 822, loss: 0.9455797672271729\n","step: 823, loss: 0.7240990400314331\n","step: 824, loss: 0.6516357660293579\n","step: 825, loss: 0.5569409728050232\n","step: 826, loss: 0.3968753516674042\n","step: 827, loss: 0.44843146204948425\n","step: 828, loss: 0.7244744300842285\n","step: 829, loss: 0.6454666256904602\n","step: 830, loss: 0.41077691316604614\n","step: 831, loss: 0.7177273631095886\n","step: 832, loss: 0.4885977506637573\n","step: 833, loss: 0.7190002202987671\n","step: 834, loss: 0.6897258162498474\n","step: 835, loss: 0.6181385517120361\n","step: 836, loss: 0.656790018081665\n","step: 837, loss: 0.8876769542694092\n","step: 838, loss: 0.5780944228172302\n","step: 839, loss: 0.6580554246902466\n","step: 840, loss: 0.49693626165390015\n","step: 841, loss: 0.976084291934967\n","step: 842, loss: 1.1665279865264893\n","step: 843, loss: 0.7634042501449585\n","step: 844, loss: 0.4592474400997162\n","step: 845, loss: 0.6521650552749634\n","step: 846, loss: 0.604257345199585\n","step: 847, loss: 0.42999404668807983\n","step: 848, loss: 0.4211183488368988\n","step: 849, loss: 0.6650140285491943\n","step: 850, loss: 0.6768314838409424\n","step: 851, loss: 0.7867262363433838\n","step: 852, loss: 0.552162766456604\n","step: 853, loss: 0.683471143245697\n","step: 854, loss: 0.5334758758544922\n","step: 855, loss: 0.3934333026409149\n","step: 856, loss: 0.5923199653625488\n","step: 857, loss: 0.7819870710372925\n","step: 858, loss: 0.5499438047409058\n","step: 859, loss: 0.724566638469696\n","step: 860, loss: 0.5917242169380188\n","step: 861, loss: 0.5821700096130371\n","step: 862, loss: 0.7010982632637024\n","step: 863, loss: 0.43105658888816833\n","step: 864, loss: 0.5886617302894592\n","step: 865, loss: 0.3731788694858551\n","step: 866, loss: 0.6861934065818787\n","step: 867, loss: 0.40124374628067017\n","step: 868, loss: 0.4997451901435852\n","step: 869, loss: 0.9123114943504333\n","step: 870, loss: 0.6429949402809143\n","step: 871, loss: 0.7613903284072876\n","step: 872, loss: 0.6411903500556946\n","step: 873, loss: 0.7453157305717468\n","step: 874, loss: 0.5561279058456421\n","step: 875, loss: 0.31352871656417847\n","step: 876, loss: 0.1813880354166031\n","step: 877, loss: 0.468698114156723\n","step: 878, loss: 0.4368137717247009\n","step: 879, loss: 0.42510750889778137\n","step: 880, loss: 0.5463407635688782\n","step: 881, loss: 0.5337278842926025\n","step: 882, loss: 0.39535972476005554\n","step: 883, loss: 0.8312969207763672\n","step: 884, loss: 0.4844611883163452\n","step: 885, loss: 0.3440937101840973\n","step: 886, loss: 0.46993958950042725\n","step: 887, loss: 0.30780550837516785\n","step: 888, loss: 0.7334935665130615\n","step: 889, loss: 0.36596229672431946\n","step: 890, loss: 0.6048087477684021\n","step: 891, loss: 0.3616228401660919\n","step: 892, loss: 0.7337977886199951\n","step: 893, loss: 0.23378139734268188\n","step: 894, loss: 0.40394097566604614\n","step: 895, loss: 0.37393414974212646\n","step: 896, loss: 0.6960070729255676\n","step: 897, loss: 0.3394729495048523\n","step: 898, loss: 0.3373357355594635\n","step: 899, loss: 0.5781334042549133\n","step: 900, loss: 0.9400162696838379\n","step: 901, loss: 0.3364957571029663\n","step: 902, loss: 0.37671539187431335\n","step: 903, loss: 0.4175991117954254\n","step: 904, loss: 0.2664114236831665\n","step: 905, loss: 0.2736276686191559\n","step: 906, loss: 0.23905211687088013\n","step: 907, loss: 0.12122467160224915\n","step: 908, loss: 0.48348358273506165\n","step: 909, loss: 0.17519031465053558\n","step: 910, loss: 0.37752318382263184\n","step: 911, loss: 0.2837793827056885\n","step: 912, loss: 0.1410055309534073\n","step: 913, loss: 0.22884833812713623\n","step: 914, loss: 0.23473875224590302\n","step: 915, loss: 0.20395444333553314\n","step: 916, loss: 0.21386711299419403\n","step: 917, loss: 0.15000879764556885\n","step: 918, loss: 0.3213382065296173\n","step: 919, loss: 0.09856383502483368\n","step: 920, loss: 0.10925284028053284\n","step: 921, loss: 0.17703770101070404\n","step: 922, loss: 0.2267451137304306\n","step: 923, loss: 0.1453864723443985\n","step: 924, loss: 0.1660711169242859\n","step: 925, loss: 0.1880970001220703\n","step: 926, loss: 0.2634926736354828\n","step: 927, loss: 0.15006718039512634\n","step: 928, loss: 0.1140180379152298\n","step: 929, loss: 0.22383300960063934\n","step: 930, loss: 0.32707661390304565\n","step: 931, loss: 0.10900440067052841\n","step: 932, loss: 0.15011489391326904\n","step: 933, loss: 0.2234017252922058\n","step: 934, loss: 0.18637916445732117\n","step: 935, loss: 0.28320586681365967\n","step: 936, loss: 0.2642483413219452\n","step: 937, loss: 0.22683483362197876\n","step: 938, loss: 0.2778639495372772\n","step: 939, loss: 0.21637172996997833\n","step: 940, loss: 0.22832143306732178\n","step: 941, loss: 0.2909255921840668\n","step: 942, loss: 0.0946895107626915\n","step: 943, loss: 0.23104077577590942\n","step: 944, loss: 0.42143958806991577\n","step: 945, loss: 0.34020355343818665\n","step: 946, loss: 0.09561231732368469\n","step: 947, loss: 0.24274875223636627\n","step: 948, loss: 0.21291448175907135\n","step: 949, loss: 0.3455122113227844\n","step: 950, loss: 0.1279243379831314\n","step: 951, loss: 0.199689120054245\n","step: 952, loss: 0.23280595242977142\n","step: 953, loss: 0.19687747955322266\n","step: 954, loss: 0.22749845683574677\n","step: 955, loss: 0.18638190627098083\n","step: 956, loss: 0.11029423028230667\n","step: 957, loss: 0.20034748315811157\n","step: 958, loss: 0.2587328255176544\n","step: 959, loss: 0.3079720437526703\n","step: 960, loss: 0.1319730430841446\n","step: 961, loss: 0.4005546569824219\n","step: 962, loss: 0.33682286739349365\n","step: 963, loss: 0.11484537273645401\n","step: 964, loss: 0.10536384582519531\n","step: 965, loss: 0.07833227515220642\n","step: 966, loss: 0.3480590879917145\n","step: 967, loss: 0.3665487766265869\n","step: 968, loss: 0.37767019867897034\n","step: 969, loss: 0.18129558861255646\n","step: 970, loss: 0.21725915372371674\n","step: 971, loss: 0.5329318046569824\n","step: 972, loss: 0.22864380478858948\n","step: 973, loss: 0.05962710082530975\n","step: 974, loss: 0.1801449954509735\n","step: 975, loss: 0.1181124597787857\n","step: 976, loss: 0.1856386512517929\n","step: 977, loss: 0.12366203963756561\n","step: 978, loss: 0.343735933303833\n","step: 979, loss: 0.141262948513031\n","step: 980, loss: 0.33406761288642883\n","step: 981, loss: 0.22445915639400482\n","step: 982, loss: 0.12011390924453735\n","step: 983, loss: 0.20940707623958588\n","step: 984, loss: 0.22052544355392456\n","step: 985, loss: 0.3090974688529968\n","step: 986, loss: 0.27287063002586365\n","step: 987, loss: 0.08790239691734314\n","step: 988, loss: 0.2570566236972809\n","step: 989, loss: 0.17381420731544495\n","step: 990, loss: 0.30625084042549133\n","step: 991, loss: 0.13054248690605164\n","step: 992, loss: 0.44866639375686646\n","step: 993, loss: 0.25328290462493896\n","step: 994, loss: 0.22711630165576935\n","step: 995, loss: 0.21396882832050323\n","step: 996, loss: 0.34401336312294006\n","step: 997, loss: 0.15340593457221985\n","step: 998, loss: 0.1615351438522339\n","step: 999, loss: 0.13962137699127197\n","step: 1000, loss: 0.06702274084091187\n","step: 1001, loss: 0.36907246708869934\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9PdTMLYnmcsR","executionInfo":{"status":"ok","timestamp":1621310172408,"user_tz":-420,"elapsed":13909,"user":{"displayName":"Zarets LE","photoUrl":"","userId":"18400968482132978129"}},"outputId":"96925653-1478-455c-9cca-d4a12ce885fd"},"source":["# Evaluate model on test data\n","test_data_reader = DataReader(\n","    data_path='/content/drive/MyDrive/Session3/20news-test-tfidf.txt',\n","    batch_size=50,\n","    vocab_size=vocab_size\n",")\n","with tf.compat.v1.Session() as sess:\n","    epoch = 4\n","\n","    trainable_variables = tf.compat.v1.trainable_variables()\n","    for variable in trainable_variables:\n","        saved_value = restore_parameters(variable.name, epoch)\n","        assign_op = variable.assign(saved_value)\n","        sess.run(assign_op)\n","\n","    num_true_preds = 0\n","    while True:\n","        test_data, test_labels = test_data_reader.next_batch()\n","        test_plabels_eval = sess.run(\n","            predicted_labels,\n","            feed_dict={\n","                mlp._X: test_data,\n","                mlp._real_Y: test_labels\n","            }\n","        )\n","        matches = np.equal(test_plabels_eval, test_labels)\n","        num_true_preds += np.sum(matches.astype(float))\n","\n","        if test_data_reader._batch_id == 0:\n","            break\n","    print('Epoch:', epoch)\n","    print('Accuracy on test data:', num_true_preds / len(test_data_reader._data))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Epoch: 4\n","Accuracy on test data: 0.7450876261285183\n"],"name":"stdout"}]}]}