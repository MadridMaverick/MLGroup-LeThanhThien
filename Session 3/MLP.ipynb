{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "wlXUUTpEMvs8",
        "outputId": "b4201848-76c1-4779-cf74-81ebcb5f5b5d"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_v2_behavior()\n",
        "\n",
        "\n",
        "class MLP:\n",
        "\tdef __init__(self, vocab_size, hidden_size):\n",
        "\t\tself._vocab_size = vocab_size\n",
        "\t\tself._hidden_size = hidden_size\n",
        "\n",
        "\tdef build_graph(self):\n",
        "\t\tself._X = tf.placeholder(tf.float32, shape = [None, self._vocab_size])\n",
        "\t\tself._real_Y = tf.placeholder(tf.int32, shape = [None, ])\n",
        "\t\t\n",
        "        weights_1 = tf.get_variable(\n",
        "\t\t\tname = 'weights_input_hidden', \n",
        "\t\t\tshape = (self._vocab_size, self._hidden_size), \n",
        "\t\t\tinitializer = tf.random_normal_initializer(seed = 2018)\n",
        "\t\t)\n",
        "\t\tbiases_1 = tf.get_variable(\n",
        "\t\t\tname = 'biases_input_hidden',\n",
        "\t\t\tshape = (self._hidden_size),\n",
        "\t\t\tinitializer = tf.random_normal_initializer(seed = 2018)\n",
        "\t\t)\n",
        "\t\t\n",
        "        weights_2 = tf.get_variable(\n",
        "\t\t\tname = 'weights_input_hidden',\n",
        "\t\t\tshape = (self._hidden_size, num_classes),\n",
        "\t\t\tinitializer = tf.random_normal_initializer(seed = 2018)\n",
        "\t\t)\n",
        "\t\tbiases_2 = tf.get_variable(\n",
        "\t\t\tname = 'biases_input_hidden',\n",
        "\t\t\tshape = (num_classes),\n",
        "\t\t\tinitializer = tf.random_normal_initializer(seed = 2018)\n",
        "\t\t)\n",
        "\t\t\n",
        "        hidden = tf.matmul(self._X, weights_1) + biases_1\n",
        "\t\thidden = tf.sigmoid(hidden)\n",
        "        logits = tf.matmul(hidden, weights_2) + biases_2\n",
        "\t\t\n",
        "        labels_one_hot = tf.one_hot(indices = self._real_Y, depth = num_classes, \n",
        "                                    dtype = tf.float32)\n",
        "\t\t\n",
        "        loss = tf.nn.softmax_cross_entropy_with_logits(labels = labels_one_hot, \n",
        "                                                       logits = logits)\n",
        "\t\t\n",
        "        loss = tf.reduce_mean(loss)\n",
        "\t\t\n",
        "        probs = tf.nn.softmax(logits)\n",
        "\t\tpredicted_labels = tf.argmax(probs, axis = 1)\n",
        "\t\tpredicted_labels = tf.squeeze(predicted_labels)\n",
        "\t\t\n",
        "        return predicted_labels, loss\n",
        "\t\n",
        "    def trainer(self, loss, learning_rate):\n",
        "\t\ttrain_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
        "\t\treturn train_op\n",
        "\n",
        "\n",
        "def load_dataset():\n",
        "\ttrain_data_reader = DataReader(\n",
        "\t\tdata_path = os.getcwd() + '/datasets/20news-bydate/20news-train-tfidf.txt', \n",
        "\t\tbatch_size = 50,\n",
        "\t\tvocab_size = vocab_size\n",
        "\t)\n",
        "\ttest_data_reader = DataReader(\n",
        "\t\tdata_path = os.getcwd() + '/datasets/20news-bydate/20news-test-tfidf.txt',\n",
        "\t\tbatch_size = 50,\n",
        "\t\tvocab_size = vocab_size\n",
        "\t)\n",
        "\treturn train_data_reader, test_data_reader\n",
        "\n",
        "class DataReader:\n",
        "\tdef __init__(self, data_path, batch_size, vocab_size):\n",
        "\t\tself._batch_size = batch_size\n",
        "\t\twith open(data_path) as f:\n",
        "\t\t\td_lines = f.read().splitlines()\n",
        "\t\tself._data = []\n",
        "\t\tself._labels = []\n",
        "\t\tfor data_id ,line in enumerate(d_lines):\n",
        "\t\t\tvector = [0.0 for _ in range(vocab_size)]\n",
        "\t\t\tfeatures = line.split('<fff>')\n",
        "\t\t\tlabel, doc_id = int(features[0]), int(features[1])\n",
        "\t\t\ttokens = features[2].split()\n",
        "\t\t\tfor token in tokens:\n",
        "\t\t\t\tindex, value = int(token.split(':')[0]), float(token.split(':')[1])\n",
        "\t\t\t\tvector[index] = value\n",
        "\t\t\tself._data.append(vector)\n",
        "\t\t\tself._labels.append(label)\n",
        "\t\tself._data = np.array(self._data)\n",
        "\t\tself._labels = np.array(self._labels)\n",
        "\t\tself._num_epoch = 0\n",
        "\t\tself._batch_id = 0\n",
        "\n",
        "\tdef next_batch(self):\n",
        "\t\tstart = self._batch_id * self._batch_size\n",
        "\t\tend = start + self._batch_size\n",
        "\t\tself._batch_id += 1\n",
        "\t\tif end + self._batch_size > len(self._data):\n",
        "\t\t\tend = len(self._data)\n",
        "\t\t\tself._num_epoch += 1\n",
        "\t\t\tself._batch_id = 0\n",
        "\t\t\tindices = range(len(self._data))\n",
        "\t\t\trandom.seed(2018)\n",
        "\t\t\trandom.shuffle(indices)\n",
        "\t\t\tself._data, self._labels = self._data[indices], self._labels[indices]\n",
        "\t\treturn self._data[start:end], self.labels[start:end]\n",
        "\n",
        "def save_parameters(name, value, epoch):\n",
        "\tfilename = name.replace(':', '-colon-') + '-epoch-{}.txt'.format(epoch)\n",
        "\tif len(value.shape) == 1:\n",
        "\t\tstring_form = ','.join([str(number) for number in value])\n",
        "\telse:\n",
        "\t\tstring_form = '\\n'.join([','.join([str(number) for number in value[row]] for row in range(value.shape[0]))])\n",
        "\twith open(os.getcwd() + '/saved-paras/' + filename, 'w') as f:\n",
        "\t\tf.write(string_form)\n",
        "\n",
        "def restore_parameters(name, epoch):\n",
        "\tfilename = name.replace(':', '-colon-') + '-epoch-{}.txt'.format(epoch)\n",
        "\twith open(os.getcwd() + '/saved-paras' + filename) as f:\n",
        "\t\tlines = f.read().splitlines()\n",
        "\t\tif len(lines) == 1:\n",
        "\t\t\tvalue = [float(number) for number in lines[0].split(',')]\n",
        "\t\telse:\n",
        "\t\t\tvalue = [[float(number) for number in lines[row].split(',')] for row in range(len(lines))]\n",
        "\treturn value\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\twith open(os.getcwd() + '/datasets/20news-bydate/words_idfs.txt') as f:\n",
        "\t\tvocab_size = len(f.read().splitlines())\n",
        "\tmlp = MLP(vocab_size = vocab_size, hidden_size = 50)\n",
        "\tpredicted_labels, loss = mlp.build_graph()\n",
        "\ttrain_op = mlp.trainer(loss = loss, learning_rate = 0.1)\n",
        "\n",
        "\twith tf.Session() as sess:\n",
        "\t\ttrain_data_reader, test_data_reader = load_dataset()\n",
        "\t\tmax_step = 1000**2\n",
        "\t\tsess.run(tf.global_variables_initializer())\n",
        "\t\tfor step in range(max_step):\n",
        "\t\t\ttrain_data, train_labels = train_data_reader.next_batch()\n",
        "\t\t\tplabels_eval, loss_eval, _ = sess.run(\n",
        "\t\t\t\t[predicted_labels, loss, train_op],\n",
        "\t\t\t\tfeed_dict={\n",
        "\t\t\t\t\tmlp._X: train_data,\n",
        "\t\t\t\t\tmlp._real_Y: train_labels\n",
        "\t\t\t\t}\n",
        "\t\t\t)\n",
        "\t\t\tprint('step: {}, loss: {}'.format(step, loss_eval))\n",
        "\t\ttrainable_variables = tf.trainable_variables()\n",
        "\t\tfor variable in trainable_variables:\n",
        "\t\t\tsave_parameters(\n",
        "\t\t\t\tname = variable.name,\n",
        "\t\t\t\tvalue = variable.eval(),\n",
        "\t\t\t\tepoch = train_data_reader._num_epoch\n",
        "\t\t\t)\n",
        "\t\t\tsaved_value = restore_parameters(variable.name, epoch)\n",
        "\t\t\tassign_op = variable.assign(saved.value)\n",
        "\t\t\tsess.run(assign_op)\n",
        "\ttest_data_reader = DataReader(\n",
        "\t\tdata_path = os.getcwd() + 'datasets/20news-bydate/20news-test-tfidf.txt',\n",
        "\t\tbatch_size = 50,\n",
        "\t\tvocab_size = vocab_size\n",
        "\t)\n",
        "\twith tf.Session() as sess:\n",
        "\t\tepoch = 10\n",
        "\t\ttrainable_variables = tf.trainable_variables()\n",
        "\t\tfor variable in trainable_variables():\n",
        "\t\t\tsaved_value = restore_parameters(variable.name,epoch)\n",
        "\t\t\tassign_op = variable.assign(saved_value)\n",
        "\t\t\tsess.run(assign.op)\n",
        "\t\tnum_true_preds = 0\n",
        "\t\twhile True:\n",
        "\t\t\ttest_data, test_labels = test_data_reader.next_batch()\n",
        "\t\t\ttest_labels_eval = sess.run(\n",
        "\t\t\t\tpredicted_labels,\n",
        "\t\t\t\tfeed_dict = {\n",
        "\t\t\t\t\tmlp._X: test_data,\n",
        "\t\t\t\t\tmlp._real_Y: test_labels\n",
        "\t\t\t\t}\n",
        "\t\t\t)\n",
        "\t\t\tmatches = np.equal(test_plabels_eval, test_labels)\n",
        "\t\t\tnum_true_preds += np.sum(matches.astype(float))\n",
        "\t\t\tif test_data_reader._batch_id == 0:\n",
        "\t\t\t\tbreak\n",
        "\t\tprint('Epoch:', epoch)\n",
        "\t\tprint('Accuracy on test data:', num_true_preds/len(test_data_reader._data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-839a292e6906>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/datasets/20news-bydate/words_idfs.txt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                 \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mmlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/datasets/20news-bydate/words_idfs.txt'"
          ]
        }
      ]
    }
  ]
}