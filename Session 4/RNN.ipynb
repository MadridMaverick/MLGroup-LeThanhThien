{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oA17WvOmOzU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa8a3973-dd51-4ab9-e595-fbacd5dbf501"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIk8WO5fr0P2"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jldxZ4L0BOi"
      },
      "source": [
        "MAX_DOC_LENGTH = 500\n",
        "NUM_CLASSES = 20"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZkYTNpQEAP2"
      },
      "source": [
        "class DataReader:\n",
        "    def __init__(self, data_path, batch_size):\n",
        "        self._batch_size = batch_size\n",
        "        with open(data_path, encoding='latin-1') as f:\n",
        "            d_lines = f.read().splitlines()\n",
        "\n",
        "        self._data = []\n",
        "        self._labels = []\n",
        "        self._sentence_lengths = []\n",
        "        self._final_tokens = []\n",
        "\n",
        "        for data_id, line in enumerate(d_lines):\n",
        "            features = line.split('<fff>')\n",
        "            label, doc_id, sentence_length = int(features[0]), int(features[1]), int(features[2])\n",
        "            tokens = features[3].split()\n",
        "\n",
        "            self._data.append(tokens)\n",
        "            self._labels.append(label)\n",
        "            self._sentence_lengths.append(sentence_length)\n",
        "            self._final_tokens.append(tokens[-1])\n",
        "\n",
        "        self._data = np.array(self._data)\n",
        "        self._labels = np.array(self._labels)\n",
        "        self._sentence_lengths = np.array(self._sentence_lengths)\n",
        "        self._final_tokens = np.array(self._final_tokens)\n",
        "\n",
        "        self._num_epoch = 0\n",
        "        self._batch_id = 0\n",
        "\n",
        "    def next_batch(self):\n",
        "        start = self._batch_id * self._batch_size\n",
        "        end = start + self._batch_size\n",
        "        self._batch_id += 1\n",
        "\n",
        "        if end + self._batch_size > len(self._data):\n",
        "            end = len(self._data)\n",
        "            start = end - self._batch_size\n",
        "\n",
        "            self._num_epoch += 1\n",
        "            self._batch_id = 0\n",
        "\n",
        "            indices = list(range(len(self._data)))\n",
        "            random.seed(2021)\n",
        "            random.shuffle(indices)\n",
        "            self._data, self._labels = self._data[indices], self._labels[indices]\n",
        "            self._sentence_lengths = self._sentence_lengths[indices]\n",
        "            self._final_tokens = self._final_tokens[indices]\n",
        "        return self._data[start:end], self._labels[start:end], self._sentence_lengths[start:end], self._final_tokens[start:end]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5ylmEbosaDs"
      },
      "source": [
        "class RNN:\n",
        "    def __init__(self, vocab_size, embedding_size, lstm_size, batch_size):\n",
        "        self._vocab_size = vocab_size\n",
        "        self._embedding_size = embedding_size\n",
        "        self._lstm_size = lstm_size\n",
        "        self._batch_size = batch_size\n",
        "\n",
        "        self._data = tf.compat.v1.placeholder(tf.int32, shape=[batch_size, MAX_DOC_LENGTH])\n",
        "        self._labels = tf.compat.v1.placeholder(tf.int32, shape=[batch_size, ])\n",
        "        self._sentence_lengths = tf.compat.v1.placeholder(tf.int32, shape=[batch_size, ])\n",
        "        self._final_token = tf.compat.v1.placeholder(tf.int32, shape=[batch_size, ])\n",
        "\n",
        "    def embedding_layer(self, indices):\n",
        "        pretrained_vector = []\n",
        "        pretrained_vector.append(np.zeros(self._embedding_size))\n",
        "        np.random.seed(2021)\n",
        "        for _ in range(self._vocab_size + 1):\n",
        "            pretrained_vector.append(np.random.normal(loc=0., scale=1., size=self._embedding_size))\n",
        "\n",
        "        pretrained_vector = np.array(pretrained_vector)\n",
        "\n",
        "        self._embedding_matrix = tf.compat.v1.get_variable(\n",
        "            name='embedding',\n",
        "            shape=(self._vocab_size + 2, self._embedding_size),\n",
        "            initializer=tf.constant_initializer(pretrained_vector)\n",
        "        )\n",
        "        return tf.nn.embedding_lookup(self._embedding_matrix, indices)\n",
        "\n",
        "    def LSTM_layer(self, embeddings):\n",
        "        lstm_cell = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(self._lstm_size)\n",
        "        zero_state = tf.zeros(shape=(self._batch_size, self._lstm_size))\n",
        "        initial_state = tf.compat.v1.nn.rnn_cell.LSTMStateTuple(zero_state, zero_state)\n",
        "\n",
        "        lstm_inputs = tf.unstack(tf.transpose(embeddings, perm=[1, 0, 2]))\n",
        "        lstm_outputs, last_state = tf.compat.v1.nn.static_rnn(\n",
        "            cell=lstm_cell,\n",
        "            inputs=lstm_inputs,\n",
        "            initial_state=initial_state,\n",
        "            sequence_length=self._sentence_lengths\n",
        "        )  # a length-500 list of [num_docs, lstm_size]\n",
        "\n",
        "        lstm_outputs = tf.unstack(tf.transpose(lstm_outputs, perm=[1, 0, 2]))\n",
        "        lstm_outputs = tf.concat(lstm_outputs, axis=0)  # [num docs * MAX_SENT_LENGTH, lstm_size]\n",
        "\n",
        "        # self.mask: [num_docs * MAX_SENT_LENGTH, ]\n",
        "        mask = tf.sequence_mask(\n",
        "            lengths=self._sentence_lengths,\n",
        "            maxlen=MAX_DOC_LENGTH,\n",
        "            dtype=tf.float32\n",
        "        )  # [num_docs, MAX_SENTENCE_LENGTH]\n",
        "        mask = tf.concat(tf.unstack(mask, axis=0), axis=0)\n",
        "        mask = tf.expand_dims(mask, -1)\n",
        "\n",
        "        lstm_outputs = mask * lstm_outputs\n",
        "        lstm_outputs_split = tf.split(lstm_outputs, num_or_size_splits=self._batch_size)\n",
        "        lstm_outputs_sum = tf.reduce_sum(lstm_outputs_split, axis=1)    # [num_docs, lstm_size]\n",
        "        lstm_outputs_average = lstm_outputs_sum / tf.expand_dims(\n",
        "            tf.cast(self._sentence_lengths, tf.float32), -1) # [num_docs, lstm_size]\n",
        "\n",
        "        return lstm_outputs_average\n",
        "\n",
        "    def build_graph(self):\n",
        "        embeddings = self.embedding_layer(self._data)\n",
        "        lstm_outputs = self.LSTM_layer(embeddings)\n",
        "\n",
        "        weights = tf.compat.v1.get_variable(\n",
        "            name='final_layer_weights',\n",
        "            shape=(self._lstm_size, NUM_CLASSES),\n",
        "            initializer=tf.random_normal_initializer(seed=2021)\n",
        "        )\n",
        "        biases = tf.compat.v1.get_variable(\n",
        "            name='final_layer_biases',\n",
        "            shape=(NUM_CLASSES),\n",
        "            initializer=tf.random_normal_initializer(seed=2021)\n",
        "        )\n",
        "        logits = tf.matmul(lstm_outputs, weights) + biases\n",
        "\n",
        "        labels_one_hot = tf.one_hot(\n",
        "            indices=self._labels,\n",
        "            depth=NUM_CLASSES,\n",
        "            dtype=tf.float32\n",
        "        )\n",
        "\n",
        "        loss = tf.nn.softmax_cross_entropy_with_logits(\n",
        "            labels=labels_one_hot,\n",
        "            logits=logits\n",
        "        )\n",
        "        loss = tf.reduce_mean(loss)\n",
        "\n",
        "        probs = tf.nn.softmax(logits)\n",
        "        predicted_labels = tf.argmax(probs, axis=1)\n",
        "        predicted_labels = tf.squeeze(predicted_labels)\n",
        "        return predicted_labels, loss\n",
        "\n",
        "    def trainer(self, loss, learning_rate):\n",
        "        train_op = tf.compat.v1.train.AdamOptimizer(learning_rate).minimize(loss)\n",
        "        return train_op"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNFmRVZdsiXG"
      },
      "source": [
        "def train_and_evaluate_RNN():\n",
        "    with open('/content/drive/MyDrive/Session4/w2v/vocab-raw.txt', encoding='latin-1') as f:\n",
        "        vocab_size = len(f.read().splitlines())\n",
        "\n",
        "    train_data_path = '/content/drive/MyDrive/Session4/w2v/20news-train-encoded.txt'\n",
        "    test_data_path = '/content/drive/MyDrive/Session4/w2v/20news-test-encoded.txt'\n",
        "\n",
        "    tf.random.set_seed(2021)\n",
        "    rnn = RNN(\n",
        "        vocab_size=vocab_size,\n",
        "        embedding_size=300,\n",
        "        lstm_size=50,\n",
        "        batch_size=50\n",
        "    )\n",
        "    predicted_labels, loss = rnn.build_graph()\n",
        "    train_op = rnn.trainer(loss=loss, learning_rate=0.01)\n",
        "\n",
        "    with tf.compat.v1.Session() as sess:\n",
        "        train_data_reader = DataReader(\n",
        "            data_path=train_data_path,\n",
        "            batch_size=50\n",
        "        )\n",
        "        test_data_reader = DataReader(\n",
        "            data_path=test_data_path,\n",
        "            batch_size=50\n",
        "        )\n",
        "        step = 0\n",
        "        MAX_STEP = 1000\n",
        "\n",
        "        sess.run(tf.compat.v1.global_variables_initializer())\n",
        "        while step < MAX_STEP:\n",
        "            next_train_batch = train_data_reader.next_batch()\n",
        "            train_data, train_labels, train_sentence_lengths, train_final_tokens = next_train_batch\n",
        "            plabels_eval, loss_eval, _ = sess.run(\n",
        "                [predicted_labels, loss, train_op],\n",
        "                feed_dict={\n",
        "                    rnn._data: train_data,\n",
        "                    rnn._labels: train_labels,\n",
        "                    rnn._sentence_lengths: train_sentence_lengths,\n",
        "                    rnn._final_token: train_final_tokens\n",
        "                }\n",
        "            )\n",
        "            step += 1\n",
        "            if step % 20 == 0:\n",
        "                print('loss:', loss_eval)\n",
        "\n",
        "            if train_data_reader._batch_id == 0:\n",
        "                num_true_preds = 0\n",
        "                while True:\n",
        "                    next_test_batch = test_data_reader.next_batch()\n",
        "                    test_data, test_labels, test_sentence_lengths, test_final_tokens = next_test_batch\n",
        "\n",
        "                    test_plabels_eval = sess.run(\n",
        "                        predicted_labels,\n",
        "                        feed_dict={\n",
        "                            rnn._data: test_data,\n",
        "                            rnn._labels: test_labels,\n",
        "                            rnn._sentence_lengths: test_sentence_lengths,\n",
        "                            rnn._final_token: test_final_tokens\n",
        "                        }\n",
        "                    )\n",
        "                    matches = np.equal(test_plabels_eval, test_labels)\n",
        "                    num_true_preds += np.sum(matches.astype(float))\n",
        "\n",
        "                    if test_data_reader._batch_id == 0:\n",
        "                        break\n",
        "                print('Epoch:', train_data_reader._num_epoch)\n",
        "                print('Accuracy on test data:', num_true_preds * 100. / len(test_data_reader._data))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNM-nI65snoj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbfcd521-b395-4c5f-8a14-c3144fc12409"
      },
      "source": [
        "tf.compat.v1.disable_eager_execution()\n",
        "train_and_evaluate_RNN()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 1.2600601\n",
            "loss: 2.7552705\n",
            "loss: 3.2598214\n",
            "loss: 0.4841204\n",
            "loss: 1.8978115\n",
            "loss: 3.4846506\n",
            "loss: 1.9305673\n",
            "loss: 1.6547973\n",
            "loss: 3.1990278\n",
            "loss: 4.3268514\n",
            "loss: 3.9816291\n",
            "Epoch: 1\n",
            "Accuracy on test data: 2.8412108337758895\n",
            "loss: 3.082207\n",
            "loss: 2.9578834\n",
            "loss: 2.8207355\n",
            "loss: 2.6995373\n",
            "loss: 2.6000586\n",
            "loss: 2.520143\n",
            "loss: 2.3366468\n",
            "loss: 2.1218448\n",
            "loss: 2.0876868\n",
            "loss: 2.0293484\n",
            "loss: 1.8363167\n",
            "Epoch: 2\n",
            "Accuracy on test data: 47.22517259691981\n",
            "loss: 1.5220538\n",
            "loss: 1.3614072\n",
            "loss: 1.1936047\n",
            "loss: 1.2628952\n",
            "loss: 1.0057307\n",
            "loss: 1.067099\n",
            "loss: 0.8837765\n",
            "loss: 0.6301675\n",
            "loss: 0.8061423\n",
            "loss: 0.7356995\n",
            "loss: 0.5982385\n",
            "Epoch: 3\n",
            "Accuracy on test data: 67.43228890069038\n",
            "loss: 0.47074273\n",
            "loss: 0.31024608\n",
            "loss: 0.5430601\n",
            "loss: 0.42074466\n",
            "loss: 0.22054943\n",
            "loss: 0.26697937\n",
            "loss: 0.25895855\n",
            "loss: 0.32652858\n",
            "loss: 0.24538912\n",
            "loss: 0.36663327\n",
            "loss: 0.45304948\n",
            "loss: 0.44568467\n",
            "Epoch: 4\n",
            "Accuracy on test data: 71.4551248008497\n",
            "loss: 0.09487627\n",
            "loss: 0.119782336\n",
            "loss: 0.1639022\n",
            "loss: 0.19712849\n",
            "loss: 0.102104604\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}